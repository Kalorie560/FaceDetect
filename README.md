# é¡”ç‰¹å¾´ç‚¹æ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ  / Facial Keypoints Detection System

[English](#english) | [æ—¥æœ¬èª](#japanese)

---

## English

### Overview
This project implements a comprehensive solution for the Kaggle Facial Keypoints Detection competition. It uses deep learning to detect 15 facial keypoints (30 coordinates) from 96x96 grayscale face images.

### Key Features
- **Multiple CNN Architectures**: BasicCNN, DeepCNN, ResNet-based, and EfficientNet-based models
- **Data Augmentation**: Robust preprocessing with rotation, scaling, brightness adjustments, and noise using Albumentations and imgaug
- **Experiment Tracking**: ClearML integration for monitoring training progress and metrics
- **Web Application**: Interactive Streamlit app with enhanced UI components for real-time keypoint detection
- **Submission Generation**: Automated generation of competition submission files in the required CSV format
- **Comprehensive Testing**: Unit tests, integration tests, and automated test runner
- **Bilingual Support**: Both Japanese and English interfaces
- **Data Exploration**: Jupyter notebook for dataset analysis and visualization
- **Configuration Management**: YAML-based configuration for flexible experiment setup

### Project Structure
```
FaceDetect/
â”œâ”€â”€ src/                   # Source code
â”‚   â”œâ”€â”€ data/             # Data processing modules
â”‚   â”‚   â”œâ”€â”€ dataset.py    # Dataset class with augmentation support
â”‚   â”‚   â””â”€â”€ preprocessing.py # Data preprocessing utilities
â”‚   â”œâ”€â”€ models/           # Model architectures
â”‚   â”‚   â””â”€â”€ cnn_model.py  # CNN models (Basic, Deep, ResNet, EfficientNet)
â”‚   â”œâ”€â”€ training/         # Training pipeline
â”‚   â”‚   â”œâ”€â”€ trainer.py    # Training class with ClearML integration
â”‚   â”‚   â””â”€â”€ train.py      # Main training script
â”‚   â””â”€â”€ utils/            # Utility modules
â”‚       â”œâ”€â”€ visualization.py # Plotting and visualization tools
â”‚       â””â”€â”€ inference.py  # Inference utilities
â”œâ”€â”€ webapp/               # Streamlit web application
â”‚   â””â”€â”€ app.py           # Main web app with bilingual interface
â”œâ”€â”€ config/               # Configuration files
â”‚   â”œâ”€â”€ clearml.yaml     # ClearML configuration template
â”‚   â””â”€â”€ training_config.yaml # Training configuration template
â”œâ”€â”€ notebooks/            # Jupyter notebooks for experiments
â”‚   â””â”€â”€ data_exploration.ipynb # Data exploration and analysis
â”œâ”€â”€ tests/                # Unit tests
â”‚   â”œâ”€â”€ test_models.py   # Model architecture tests
â”‚   â””â”€â”€ test_data.py     # Data processing tests
â”œâ”€â”€ run_tests.py          # Integrated test runner script
â”œâ”€â”€ test_clearml_integration.py # ClearML integration tests
â”œâ”€â”€ test_config_loading.py     # Configuration loading tests
â”œâ”€â”€ test_fix.py           # Additional test utilities
â”œâ”€â”€ test_json_fix.py      # JSON handling test utilities
â”œâ”€â”€ test_transforms.py    # Data transformation tests
â”œâ”€â”€ generate_submission.py # Submission file generation script
â”œâ”€â”€ test_submission_generator.py # Tests for submission generator
â”œâ”€â”€ examples/             # Example scripts and tutorials
â”‚   â””â”€â”€ submission_example.py # Submission generation examples
â”œâ”€â”€ test_training.csv     # Sample training data for testing
â”œâ”€â”€ verify_fixes.py       # Fix verification script
â”œâ”€â”€ requirements.txt      # Python dependencies
â””â”€â”€ README.md            # This file
```

### Data Preprocessing Details

#### Normalization
- Images normalized to [0, 1] range with ImageNet statistics (mean=0.485, std=0.229)
- Keypoints normalized to [0, 1] relative to image dimensions

#### Data Augmentation
- **Geometric**: Horizontal flip (50%), rotation (Â±15Â°), shift-scale-rotate
- **Photometric**: Random brightness/contrast, Gaussian noise, Gaussian blur
- **Keypoint-aware**: All augmentations preserve keypoint relationships

#### Missing Value Handling
- **Drop**: Remove samples with missing keypoints (default)
- **Interpolate**: Use temporal interpolation for missing values
- **Zero**: Fill missing values with zeros

### Model Architectures

#### BasicCNN
- 4 convolutional blocks with batch normalization
- MaxPooling and dropout for regularization
- 2 fully connected layers
- **Parameters**: ~2.3M

#### DeepCNN
- 5 convolutional blocks with residual-like connections
- Adaptive global average pooling
- Enhanced feature extraction
- **Parameters**: ~8.1M

#### ResNet-based
- Pre-trained ResNet backbones (ResNet18/34/50)
- Modified first layer for grayscale input
- Custom head for keypoint regression
- **Parameters**: 11.2M - 23.5M

#### EfficientNet-based
- Pre-trained EfficientNet backbones (B0/B2)
- Efficient architecture with compound scaling
- Modified for grayscale input
- **Parameters**: 4.0M - 7.7M

### Training Configuration

#### Loss Function
- **MSE Loss**: Mean Squared Error for coordinate regression
- **Alternative**: L1 Loss, Smooth L1 Loss available

#### Optimizer
- **Adam**: Default with learning rate 0.001
- **AdamW**: With weight decay for better generalization
- **SGD**: With momentum for stable training

#### Learning Rate Scheduling
- **StepLR**: Decay by factor of 0.1 every 30 epochs
- **CosineAnnealingLR**: Smooth decay to zero
- **ReduceLROnPlateau**: Adaptive reduction based on validation loss

### Setup Instructions

#### 1. Environment Setup
```bash
# Clone the repository
git clone https://github.com/Kalorie560/FaceDetect.git
cd FaceDetect

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\\Scripts\\activate

# Install dependencies
pip install -r requirements.txt
```

#### 2. Configuration Setup

##### ClearML Configuration
```bash
# Edit the ClearML configuration file with your credentials
# config/clearml.yaml:
# - Replace YOUR_ACCESS_KEY_HERE with your ClearML access key
# - Replace YOUR_SECRET_KEY_HERE with your ClearML secret key
# - Update the project_name and experiment settings as needed

# The system uses config/clearml.yaml by default
# You can specify a different config file using the --clearml_config flag
```

##### Training Configuration
```bash
# The training_config.yaml file provides comprehensive training settings
# You can modify parameters such as:
# - Model type (resnet18, efficientnet_b0, etc.)
# - Training hyperparameters (epochs, batch_size, learning_rate)
# - Data processing options (augmentation, missing value handling)
# - Hardware settings (device, mixed precision)

# Copy and customize for your experiments
cp config/training_config.yaml config/my_training_config.yaml
```

#### 3. Data Preparation

##### Dataset File Placement
Place the competition dataset files in the project root directory:
```
FaceDetect/
â”œâ”€â”€ training.csv        # Training data with images and keypoints
â”œâ”€â”€ test.csv           # Test data with images only
â”œâ”€â”€ IdLookupTable.csv  # Maps image IDs to keypoint labels
â””â”€â”€ ... (other project files)
```

##### Download and Setup
```bash
# Download Kaggle data
kaggle competitions download -c facial-keypoints-detection

# Extract data to project root
unzip facial-keypoints-detection.zip -d .

# Verify files are in the correct location
ls -la *.csv
# Should show: training.csv, test.csv, IdLookupTable.csv

# The training.csv file should contain:
# - 'Image' column with space-separated pixel values
# - 30 keypoint coordinate columns
```

### Data Exploration

Before training, you can explore the dataset using the provided Jupyter notebook:

```bash
# Start Jupyter notebook
jupyter notebook

# Open notebooks/data_exploration.ipynb
# This notebook provides:
# - Dataset statistics and visualization
# - Missing value analysis
# - Sample image visualization with keypoints
# - Data distribution analysis
# - Train/validation/test split validation
# - Comprehensive exploratory data analysis in both Japanese and English
```

### Training the Model

#### Configuration-based Training
```bash
# Use the training configuration file for comprehensive setup
python src/training/train.py \
    --config config/training_config.yaml
```

#### Basic Training
```bash
python src/training/train.py \
    --data_path ./training.csv \
    --model_type resnet18 \
    --epochs 100 \
    --batch_size 32 \
    --learning_rate 0.001
```

#### Advanced Training with ClearML
```bash
python src/training/train.py \
    --data_path ./training.csv \
    --model_type efficientnet_b0 \
    --epochs 150 \
    --batch_size 64 \
    --clearml_config config/clearml.yaml \
    --project_name "facial_keypoints_detection" \
    --experiment_name "efficientnet_b0_experiment"

# Or use the default config file (config/clearml.yaml)
python src/training/train.py \
    --data_path ./training.csv \
    --model_type efficientnet_b0 \
    --epochs 150 \
    --batch_size 64 \
    --project_name "facial_keypoints_detection" \
    --experiment_name "efficientnet_b0_experiment"
```

### Running the Web Application

```bash
# Start the Streamlit app
streamlit run webapp/app.py

# Open browser to http://localhost:8501
# Upload an image and detect facial keypoints in real-time
```

### Web Application Features

#### Input Requirements
- **Supported Formats**: JPEG, PNG
- **Recommended Size**: 96x96 pixels or larger
- **Optimal Input**: Front-facing images with face centered
- **Single Face**: Works best with one face per image

#### Real-time Detection
- Upload image through web interface
- Automatic face detection and cropping (optional)
- Real-time keypoint prediction and visualization
- Confidence score display
- Coordinate values table
- Results download in JSON format

### Generating Submission Files

Generate submission files for the Kaggle competition using trained models.

#### Quick Start - Simple Execution

```bash
# Simply run the script - no arguments needed!
python generate_submission.py
```

The script will automatically:
- ğŸ” **Find trained models** - Searches for `.pth` files in the project directory
- ğŸ¤– **Detect model type** - Automatically identifies model architecture from filename
- ğŸ“‹ **Generate format** - Creates the required submission format without needing `submissionFileFormat.csv`
- ğŸš€ **Create submission** - Generates `submission.csv` ready for Kaggle upload

#### Required Files

Before generating submissions, ensure you have:

1. **Trained Model**: Any `.pth` file with trained model weights (script will auto-detect the most recent one)
2. **Test Data**: `test.csv` from Kaggle competition (1783 test images)

```bash
# Download competition data using Kaggle CLI
kaggle competitions download -c facial-keypoints-detection

# Extract files
unzip facial-keypoints-detection.zip
```

#### Model Type Auto-Detection

The script automatically detects model type based on filename:

- **ResNet models**: `resnet18_model.pth`, `my_resnet34.pth`, `resnet50_best.pth`
- **EfficientNet models**: `efficientnet_b0.pth`, `efficient_b2_trained.pth`
- **CNN models**: `basic_cnn.pth`, `deep_cnn_model.pth`

If detection fails, it defaults to ResNet18.

#### Programmatic Usage

```python
from src.utils.inference import SubmissionGenerator, KeypointsPredictor
from src.models.cnn_model import create_model

# Create model and predictor
model = create_model('resnet18', num_keypoints=30)
predictor = KeypointsPredictor(model=model, model_path='my_model.pth')

# Create submission generator
submission_generator = SubmissionGenerator(predictor)

# Generate submission file with auto-format
output_path = submission_generator.generate_submission_file(
    test_csv_file='test.csv',
    submission_format_file='temp_format.csv',  # Auto-generated
    output_file='submission.csv'
)
```

#### Submission Format

The generated CSV file follows the competition requirements:

```csv
RowId,ImageId,FeatureName,Location
1,1,left_eye_center_x,37.5
2,1,left_eye_center_y,32.4
3,1,right_eye_center_x,59.6
4,1,right_eye_center_y,32.8
...
```

- **RowId**: Sequential ID for each prediction
- **ImageId**: Test image ID (1-1783)
- **FeatureName**: Facial keypoint name (e.g., 'left_eye_center_x')
- **Location**: Predicted coordinate value

#### Supported Models

All trained model architectures are automatically supported:

- **BasicCNN**: Lightweight 4-layer CNN
- **DeepCNN**: Enhanced 5-layer CNN with residual connections
- **ResNet**: ResNet18, ResNet34, ResNet50 variants
- **EfficientNet**: EfficientNet-B0, EfficientNet-B2 variants

Simply place your trained `.pth` file in the project directory and run the script!

#### Testing Submission Generator

```bash
# Test the submission generator functionality
python test_submission_generator.py

# Run example scripts
python examples/submission_example.py
```

### Results and Evaluation

#### Performance Metrics
- **MSE**: Mean Squared Error for coordinate accuracy
- **MAE**: Mean Absolute Error for robust evaluation
- **Per-keypoint Analysis**: Individual keypoint accuracy assessment

#### Validation Strategy
- **Train/Val/Test Split**: 70%/20%/10% default
- **Stratified Sampling**: Ensures balanced data distribution
- **Cross-validation**: K-fold validation for robust evaluation

#### Expected Performance
- **BasicCNN**: MSE ~0.005-0.01 on validation set
- **ResNet18**: MSE ~0.003-0.007 on validation set
- **EfficientNet-B0**: MSE ~0.002-0.005 on validation set

### Testing

#### Quick Test Runner
```bash
# Run the integrated test suite
python run_tests.py

# This script performs:
# - Python syntax validation
# - Module import tests
# - Unit tests with pytest (if available)
# - Requirements validation
```

#### Detailed Testing
```bash
# Run all tests with pytest
python -m pytest tests/

# Run specific test modules
python -m pytest tests/test_models.py
python -m pytest tests/test_data.py

# Run additional test files
python test_clearml_integration.py
python test_config_loading.py
python test_transforms.py
python test_json_fix.py

# Verify fixes and validate project state
python verify_fixes.py

# Run with coverage
pip install pytest-cov
python -m pytest --cov=src tests/
```

### Additional Considerations

#### Hardware Requirements
- **GPU**: CUDA-compatible GPU recommended for training
- **Memory**: 8GB+ RAM, 4GB+ GPU memory
- **Storage**: 2GB+ for models and data

#### Performance Optimization
- **Mixed Precision**: Use `--mixed_precision` flag for faster training
- **Data Loading**: Adjust `--num_workers` based on CPU cores
- **Batch Size**: Increase based on available GPU memory

#### Security Best Practices
- Never commit API keys or credentials to the repository
- Use environment variables or secure config files
- Implement proper input validation in the web application

#### Project Maintenance
- Use `verify_fixes.py` to validate project state after changes
- Run comprehensive test suite before deploying
- Check `test_training.csv` for sample data format validation

### Contributing
1. Fork the repository
2. Create a feature branch
3. Make your changes with tests
4. Submit a pull request

### License
This project is open source and available under the MIT License.

---

## Japanese

### æ¦‚è¦
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã€Kaggleé¡”ç‰¹å¾´ç‚¹æ¤œå‡ºã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®åŒ…æ‹¬çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè£…ã—ã¦ã„ã¾ã™ã€‚æ·±å±¤å­¦ç¿’ã‚’ä½¿ç”¨ã—ã¦ã€96x96ã®ã‚°ãƒ¬ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«é¡”ç”»åƒã‹ã‚‰15å€‹ã®é¡”ç‰¹å¾´ç‚¹ï¼ˆ30åº§æ¨™ï¼‰ã‚’æ¤œå‡ºã—ã¾ã™ã€‚

### ä¸»è¦æ©Ÿèƒ½
- **è¤‡æ•°ã®CNNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: BasicCNNã€DeepCNNã€ResNetãƒ™ãƒ¼ã‚¹ã€EfficientNetãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«
- **ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µ**: Albumentationsã¨imgaugã«ã‚ˆã‚‹å›è»¢ã€ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ã€æ˜åº¦èª¿æ•´ã€ãƒã‚¤ã‚ºã‚’å«ã‚€å …ç‰¢ãªå‰å‡¦ç†
- **å®Ÿé¨“è¿½è·¡**: å­¦ç¿’é€²è¡Œã¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ç›£è¦–ã®ãŸã‚ã®ClearMLçµ±åˆ
- **Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³**: å¼·åŒ–ã•ã‚ŒãŸUIã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã‚’æŒã¤ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç‰¹å¾´ç‚¹æ¤œå‡ºã®ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªStreamlitã‚¢ãƒ—ãƒª
- **åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆ**: ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆã€çµ±åˆãƒ†ã‚¹ãƒˆã€è‡ªå‹•ãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼
- **ãƒã‚¤ãƒªãƒ³ã‚¬ãƒ«ã‚µãƒãƒ¼ãƒˆ**: æ—¥æœ¬èªã¨è‹±èªã®ä¸¡æ–¹ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹
- **ãƒ‡ãƒ¼ã‚¿æ¢ç´¢**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆåˆ†æã¨å¯è¦–åŒ–ã®ãŸã‚ã®Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯
- **è¨­å®šç®¡ç†**: æŸ”è»Ÿãªå®Ÿé¨“è¨­å®šã®ãŸã‚ã®YAMLãƒ™ãƒ¼ã‚¹è¨­å®š

### ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã®è©³ç´°

#### æ­£è¦åŒ–æ‰‹æ³•
- ç”»åƒã‚’[0, 1]ç¯„å›²ã«ImageNetçµ±è¨ˆï¼ˆå¹³å‡=0.485ã€æ¨™æº–åå·®=0.229ï¼‰ã§æ­£è¦åŒ–
- ç‰¹å¾´ç‚¹ã‚’ç”»åƒã‚µã‚¤ã‚ºã«å¯¾ã—ã¦[0, 1]ã§æ­£è¦åŒ–

#### ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã®ç¨®é¡ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **å¹¾ä½•å­¦çš„å¤‰æ›**: æ°´å¹³ãƒ•ãƒªãƒƒãƒ—ï¼ˆ50%ï¼‰ã€å›è»¢ï¼ˆÂ±15Â°ï¼‰ã€ã‚·ãƒ•ãƒˆãƒ»ã‚¹ã‚±ãƒ¼ãƒ«ãƒ»å›è»¢
- **ãƒ•ã‚©ãƒˆãƒ¡ãƒˆãƒªãƒƒã‚¯**: ãƒ©ãƒ³ãƒ€ãƒ æ˜åº¦/ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆã€ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒã‚¤ã‚ºã€ã‚¬ã‚¦ã‚·ã‚¢ãƒ³ãƒ–ãƒ©ãƒ¼
- **ç‰¹å¾´ç‚¹å¯¾å¿œ**: ã™ã¹ã¦ã®æ‹¡å¼µã§ç‰¹å¾´ç‚¹ã®é–¢ä¿‚ã‚’ä¿æŒ

#### æ¬ æå€¤å‡¦ç†ã®æˆ¦ç•¥
- **å‰Šé™¤**: æ¬ æç‰¹å¾´ç‚¹ã®ã‚ã‚‹ã‚µãƒ³ãƒ—ãƒ«ã‚’å‰Šé™¤ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- **è£œé–“**: æ¬ æå€¤ã«å¯¾ã—ã¦æ™‚ç³»åˆ—è£œé–“ã‚’ä½¿ç”¨
- **ã‚¼ãƒ­è£œå®Œ**: æ¬ æå€¤ã‚’ã‚¼ãƒ­ã§åŸ‹ã‚ã‚‹

### ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è©³ç´°èª¬æ˜

#### BasicCNN ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ 
```
Input (1, 96, 96)
â”œâ”€â”€ Conv2d(1â†’32, 3x3) + BatchNorm + ReLU + MaxPool(2x2)
â”œâ”€â”€ Conv2d(32â†’64, 3x3) + BatchNorm + ReLU + MaxPool(2x2)
â”œâ”€â”€ Conv2d(64â†’128, 3x3) + BatchNorm + ReLU + MaxPool(2x2)
â”œâ”€â”€ Conv2d(128â†’256, 3x3) + BatchNorm + ReLU + MaxPool(2x2)
â”œâ”€â”€ Flatten â†’ Linear(9216â†’1024) + ReLU + Dropout(0.5)
â”œâ”€â”€ Linear(1024â†’512) + ReLU + Dropout(0.5)
â””â”€â”€ Linear(512â†’30) [å‡ºåŠ›å±¤]
```

### å­¦ç¿’è¨­å®š

#### æå¤±é–¢æ•°
- **MSE Loss**: åº§æ¨™å›å¸°ã®ãŸã‚ã®å¹³å‡äºŒä¹—èª¤å·®
- **ä»£æ›¿æ¡ˆ**: L1 Lossã€Smooth L1 Lossã‚‚åˆ©ç”¨å¯èƒ½

#### ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
- **Adam**: å­¦ç¿’ç‡0.001ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
- **é‡ã¿æ¸›è¡°**: æ±åŒ–æ€§èƒ½å‘ä¸Šã®ãŸã‚ã®AdamW
- **é‹å‹•é‡**: å®‰å®šã—ãŸå­¦ç¿’ã®ãŸã‚ã®SGD

#### å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©
- **StepLR**: 30ã‚¨ãƒãƒƒã‚¯ã”ã¨ã«0.1å€ã«æ¸›è¡°
- **CosineAnnealingLR**: ã‚¼ãƒ­ã¾ã§ã®æ»‘ã‚‰ã‹ãªæ¸›è¡°
- **ReduceLROnPlateau**: æ¤œè¨¼æå¤±ã«åŸºã¥ãé©å¿œçš„æ¸›å°‘

### ç’°å¢ƒæ§‹ç¯‰æ‰‹é †

#### 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# ãƒªãƒã‚¸ãƒˆãƒªã®ã‚¯ãƒ­ãƒ¼ãƒ³
git clone https://github.com/Kalorie560/FaceDetect.git
cd FaceDetect

# ä»®æƒ³ç’°å¢ƒã®ä½œæˆ
python -m venv venv
source venv/bin/activate  # Windows: venv\\Scripts\\activate

# ä¾å­˜é–¢ä¿‚ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install -r requirements.txt
```

#### 2. ClearMLè¨­å®šæ–¹æ³•
```bash
# ClearMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èªè¨¼æƒ…å ±ã§ç·¨é›†
# config/clearml.yaml:
# - YOUR_ACCESS_KEY_HEREã‚’ClearMLã‚¢ã‚¯ã‚»ã‚¹ã‚­ãƒ¼ã«ç½®æ›
# - YOUR_SECRET_KEY_HEREã‚’ClearMLã‚·ãƒ¼ã‚¯ãƒ¬ãƒƒãƒˆã‚­ãƒ¼ã«ç½®æ›
# - project_nameã¨å®Ÿé¨“è¨­å®šã‚’å¿…è¦ã«å¿œã˜ã¦æ›´æ–°

# ã‚·ã‚¹ãƒ†ãƒ ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§config/clearml.yamlã‚’ä½¿ç”¨ã—ã¾ã™
# --clearml_configãƒ•ãƒ©ã‚°ã§åˆ¥ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æŒ‡å®šã§ãã¾ã™
```

#### 3. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™

##### ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã®é…ç½®å ´æ‰€
ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ãã ã•ã„:
```
FaceDetect/
â”œâ”€â”€ training.csv        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒã¨ç‰¹å¾´ç‚¹ï¼‰
â”œâ”€â”€ test.csv           # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆç”»åƒã®ã¿ï¼‰
â”œâ”€â”€ IdLookupTable.csv  # ç”»åƒIDã¨ç‰¹å¾´ç‚¹ãƒ©ãƒ™ãƒ«ã®ãƒãƒƒãƒ”ãƒ³ã‚°
â””â”€â”€ ... (ãã®ä»–ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ•ã‚¡ã‚¤ãƒ«)
```

##### ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã¨ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
```bash
# Kaggleãƒ‡ãƒ¼ã‚¿ã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
kaggle competitions download -c facial-keypoints-detection

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’å±•é–‹
unzip facial-keypoints-detection.zip -d .

# ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ã„å ´æ‰€ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
ls -la *.csv
# çµæœ: training.csv, test.csv, IdLookupTable.csv ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¯ãš

# training.csvã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¦ã„ã¾ã™:
# - 'Image'åˆ—: ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã®ç”»ç´ å€¤
# - 30å€‹ã®ç‰¹å¾´ç‚¹åº§æ¨™åˆ—
```

### ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ã®å®Ÿè¡Œæ–¹æ³•

#### åŸºæœ¬çš„ãªå­¦ç¿’
```bash
python src/training/train.py \
    --data_path ./training.csv \
    --model_type resnet18 \
    --epochs 100 \
    --batch_size 32 \
    --learning_rate 0.001
```

### Webã‚¢ãƒ—ãƒªã®èµ·å‹•æ–¹æ³•

```bash
# Streamlitã‚¢ãƒ—ãƒªã®èµ·å‹•
streamlit run webapp/app.py

# ãƒ–ãƒ©ã‚¦ã‚¶ã§http://localhost:8501ã‚’é–‹ã
# ç”»åƒã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§é¡”ç‰¹å¾´ç‚¹ã‚’æ¤œå‡º
```

### çµæœã¨è©•ä¾¡

#### æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§ã®æ€§èƒ½
- **BasicCNN**: æ¤œè¨¼ã‚»ãƒƒãƒˆã§MSEç´„0.005-0.01
- **ResNet18**: æ¤œè¨¼ã‚»ãƒƒãƒˆã§MSEç´„0.003-0.007
- **EfficientNet-B0**: æ¤œè¨¼ã‚»ãƒƒãƒˆã§MSEç´„0.002-0.005

#### äºˆæ¸¬çµæœã®å¯è¦–åŒ–ä¾‹
- Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–
- ç‰¹å¾´ç‚¹ã®åº§æ¨™å€¤è¡¨ç¤º
- ä¿¡é ¼åº¦ã‚¹ã‚³ã‚¢ã®è¡¨ç¤º
- JSONå½¢å¼ã§ã®çµæœãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

### æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ

å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ç”¨ã®æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã¾ã™ã€‚

#### ç°¡å˜å®Ÿè¡Œ - å¼•æ•°ä¸è¦

```bash
# ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ - å¼•æ•°ã¯ä¸è¦ã§ã™ï¼
python generate_submission.py
```

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯è‡ªå‹•çš„ã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™ï¼š
- ğŸ” **ãƒ¢ãƒ‡ãƒ«æ¤œç´¢** - ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®`.pth`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
- ğŸ¤– **ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥æ¤œå‡º** - ãƒ•ã‚¡ã‚¤ãƒ«åã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’è‡ªå‹•è­˜åˆ¥
- ğŸ“‹ **ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆç”Ÿæˆ** - `submissionFileFormat.csv`ã‚’å¿…è¦ã¨ã›ãšã«å¿…è¦ãªæå‡ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã‚’ä½œæˆ
- ğŸš€ **æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ** - Kaggleã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æº–å‚™å®Œäº†ã®`submission.csv`ã‚’ç”Ÿæˆ

#### å¿…è¦ãƒ•ã‚¡ã‚¤ãƒ«

æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå‰ã«ä»¥ä¸‹ã‚’ç¢ºä¿ã—ã¦ãã ã•ã„ï¼š

1. **å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«é‡ã¿ã®`.pth`ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒæœ€æ–°ã®ã‚‚ã®ã‚’è‡ªå‹•æ¤œå‡ºï¼‰
2. **ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿**: Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®`test.csv`ï¼ˆ1783ãƒ†ã‚¹ãƒˆç”»åƒï¼‰

```bash
# Kaggle CLIã§ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
kaggle competitions download -c facial-keypoints-detection

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å±•é–‹
unzip facial-keypoints-detection.zip
```

#### ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã®è‡ªå‹•æ¤œå‡º

ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ãƒ•ã‚¡ã‚¤ãƒ«åã«åŸºã¥ã„ã¦ãƒ¢ãƒ‡ãƒ«ç¨®åˆ¥ã‚’è‡ªå‹•æ¤œå‡ºã—ã¾ã™ï¼š

- **ResNetãƒ¢ãƒ‡ãƒ«**: `resnet18_model.pth`, `my_resnet34.pth`, `resnet50_best.pth`
- **EfficientNetãƒ¢ãƒ‡ãƒ«**: `efficientnet_b0.pth`, `efficient_b2_trained.pth`
- **CNNãƒ¢ãƒ‡ãƒ«**: `basic_cnn.pth`, `deep_cnn_model.pth`

æ¤œå‡ºã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ResNet18ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚

#### ã‚µãƒãƒ¼ãƒˆã•ã‚Œã‚‹ãƒ¢ãƒ‡ãƒ«

å…¨ã¦ã®å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãŒè‡ªå‹•çš„ã«ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¾ã™ï¼š

- **BasicCNN**: è»½é‡4å±¤CNN
- **DeepCNN**: æ®‹å·®æ¥ç¶šã‚’æŒã¤å¼·åŒ–5å±¤CNN
- **ResNet**: ResNet18ã€ResNet34ã€ResNet50ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
- **EfficientNet**: EfficientNet-B0ã€EfficientNet-B2ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³

å­¦ç¿’æ¸ˆã¿`.pth`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¦ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’å®Ÿè¡Œã™ã‚‹ã ã‘ã§ã™ï¼

#### æå‡ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ

ç”Ÿæˆã•ã‚Œã‚‹CSVãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³è¦ä»¶ã«å¾“ã„ã¾ã™ï¼š

```csv
RowId,ImageId,FeatureName,Location
1,1,left_eye_center_x,37.5
2,1,left_eye_center_y,32.4
3,1,right_eye_center_x,59.6
4,1,right_eye_center_y,32.8
...
```

### ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

#### çµ±åˆãƒ†ã‚¹ãƒˆãƒ©ãƒ³ãƒŠãƒ¼
```bash
# çµ±åˆãƒ†ã‚¹ãƒˆã‚¹ã‚¤ãƒ¼ãƒˆã‚’å®Ÿè¡Œ
python run_tests.py

# ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¾ã™:
# - Pythonæ§‹æ–‡ã®æ¤œè¨¼
# - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
# - ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆï¼ˆpytestãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆï¼‰
# - ä¾å­˜é–¢ä¿‚ã®æ¤œè¨¼
```

#### è©³ç´°ãƒ†ã‚¹ãƒˆ
```bash
# pytestã§ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python -m pytest tests/

# ç‰¹å®šã®ãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®Ÿè¡Œ
python -m pytest tests/test_models.py
python -m pytest tests/test_data.py

# è¿½åŠ ã®ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å®Ÿè¡Œ
python test_clearml_integration.py
python test_config_loading.py
python test_transforms.py
python test_json_fix.py

# ä¿®æ­£æ¤œè¨¼ã¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆçŠ¶æ…‹ã®ç¢ºèª
python verify_fixes.py
```

### è¿½åŠ ã®è€ƒæ…®äº‹é …

#### ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢è¦ä»¶
- **GPU**: å­¦ç¿’ã«ã¯CUDAå¯¾å¿œGPUã‚’æ¨å¥¨
- **ãƒ¡ãƒ¢ãƒª**: 8GBä»¥ä¸Šã®RAMã€4GBä»¥ä¸Šã®GPUãƒ¡ãƒ¢ãƒª
- **ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸**: ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒ¼ã‚¿ç”¨ã«2GBä»¥ä¸Š

#### ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- APIã‚­ãƒ¼ã‚„èªè¨¼æƒ…å ±ã‚’ãƒªãƒã‚¸ãƒˆãƒªã«ã‚³ãƒŸãƒƒãƒˆã—ãªã„
- ç’°å¢ƒå¤‰æ•°ã‚„ã‚»ã‚­ãƒ¥ã‚¢ãªè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½¿ç”¨
- Webã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã§é©åˆ‡ãªå…¥åŠ›æ¤œè¨¼ã‚’å®Ÿè£…

### ãƒ©ã‚¤ã‚»ãƒ³ã‚¹
ã“ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã¯ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ã§ã€MITãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã®ä¸‹ã§åˆ©ç”¨å¯èƒ½ã§ã™ã€‚